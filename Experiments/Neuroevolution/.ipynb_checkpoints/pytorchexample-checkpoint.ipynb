{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhanabi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "#import checkpointer\n",
    "#import iteration_statistics\n",
    "#import dqn_agent\n",
    "#import gin.tf\n",
    "import rl_env\n",
    "import numpy as np\n",
    "#import rainbow_agent\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import statistics\n",
    "import pandas as pd\n",
    "\n",
    "import rl_env\n",
    "#import run_paired_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_encoded_observations(encoder, state, num_players):\n",
    "    print(\"--- EncodedObservations ---\")\n",
    "    print(\"Observation encoding shape: {}\".format(encoder.shape()))\n",
    "    print(\"Current actual player: {}\".format(state.cur_player()))\n",
    "    for i in range(num_players):\n",
    "      print(\"Encoded observation for player {}: {}\".format(\n",
    "          i, encoder.encode(state.observation(i))))\n",
    "    print(\"--- EndEncodedObservations ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_observations(encoder, state, player_num):\n",
    "    codes = encoder.encode(state.observation(player_num))\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_observation(observation):\n",
    "    \"\"\"Print some basic information about an agent observation.\"\"\"\n",
    "    print(\"--- Observation ---\")\n",
    "    print(observation)\n",
    "\n",
    "    print(\"### Information about the observation retrieved separately ###\")\n",
    "    print(\"### Current player, relative to self: {}\".format(\n",
    "        observation.cur_player_offset()))\n",
    "    print(\"### Observed hands: {}\".format(observation.observed_hands()))\n",
    "    print(\"### Card knowledge: {}\".format(observation.card_knowledge()))\n",
    "    print(\"### Discard pile: {}\".format(observation.discard_pile()))\n",
    "    print(\"### Fireworks: {}\".format(observation.fireworks()))\n",
    "    print(\"### Deck size: {}\".format(observation.deck_size()))\n",
    "    move_string = \"### Last moves:\"\n",
    "    for move_tuple in observation.last_moves():\n",
    "      move_string += \" {}\".format(move_tuple)\n",
    "    print(move_string)\n",
    "    print(\"### Information tokens: {}\".format(observation.information_tokens()))\n",
    "    print(\"### Life tokens: {}\".format(observation.life_tokens()))\n",
    "    print(\"### Legal moves: {}\".format(observation.legal_moves()))\n",
    "    print(\"--- EndObservation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state(state):\n",
    "    \"\"\"Print some basic information about the state.\"\"\"\n",
    "    print(\"\")\n",
    "    print(\"Current player: {}\".format(state.cur_player()))\n",
    "    print(state)\n",
    "\n",
    "    # Example of more queries to provide more about this state. For\n",
    "    # example, bots could use these methods to to get information\n",
    "    # about the state in order to act accordingly.\n",
    "    print(\"### Information about the state retrieved separately ###\")\n",
    "    print(\"### Information tokens: {}\".format(state.information_tokens()))\n",
    "    print(\"### Life tokens: {}\".format(state.life_tokens()))\n",
    "    print(\"### Fireworks: {}\".format(state.fireworks()))\n",
    "    print(\"### Deck size: {}\".format(state.deck_size()))\n",
    "    print(\"### Discard pile: {}\".format(str(state.discard_pile())))\n",
    "    print(\"### Player hands: {}\".format(str(state.player_hands())))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from rl_env import Agent\n",
    "\n",
    "\n",
    "class RandomAgent():\n",
    "    def act(self, observation):\n",
    "        \"\"\"Act based on an observation.\"\"\"\n",
    "        return random.choice(observation.legal_moves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent = RandomAgent()\n",
    "agent = RandomAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=658, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (fc3): Linear(in_features=60, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        #self.conv1 = nn.Conv2d(2, 2, 13)\n",
    "        #self.conv2 = nn.Conv2d(2, 2, 1)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(658, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.fc3 = nn.Linear(60, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        #x = F.max_pool2d(F.relu(self.conv1(x)), (1, 1))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        #x = F.max_pool2d(F.relu(self.conv2(x)), 1)\n",
    "        #x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \"\"\"\n",
    "    def foward2(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \"\"\"\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_tensor(mylist):\n",
    "    encoded_observations = np.array(mylist).reshape(1,658)\n",
    "    tensor_observations = torch.FloatTensor(encoded_observations)\n",
    "    return tensor_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_encoded_observations = get_encoded_observations(obs_encoder, state, 0)\n",
    "#encoded_observations = np.array(list_encoded_observations).reshape(1,658)\n",
    "#tensor_observations = torch.FloatTensor(encoded_observations)\n",
    "#net(tensor_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = tensor_observations\n",
    "#out = net(input)\n",
    "#print(out)\n",
    "#net.zero_grad()\n",
    "#out.backward(torch.randn(1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.optim as optim\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def make_action(array):\\n        index = np.rgmax(array)\\n        return index\\n\\n\\n    def get_weights():\\n        #Pytorch code to get an array with all the weights of the network\\n        weights  = [] #np.zeros()\\n\\n    def set_wegiths(weights):\\n        #Pytorch code to set the weigths\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PytorchAgent:\n",
    "\n",
    "    def __init__(self,game):\n",
    "        self.net = Net()\n",
    "        self.game = game\n",
    "        self.obs_encoder = pyhanabi.ObservationEncoder(game, enc_type=pyhanabi.ObservationEncoderType.CANONICAL)\n",
    "\n",
    "    def act(self, state):\n",
    "        encoded_observations = get_encoded_observations(self.obs_encoder, state, state.cur_player())\n",
    "        input_tensor = list_to_tensor(encoded_observations)\n",
    "        output_tensor = self.net(input_tensor)\n",
    "        weight = output_tensor.tolist()[0]#weight of 20 possible moves\n",
    "        rank = sorted(range(len(weight)), key=lambda k: weight[k]) # rank of 20 possible moves in weight\n",
    "        index = sorted(range(len(rank)), key=lambda k: rank[k]) # index of sorted rank of 20 possible moves\n",
    "        observation = state.observation(state.cur_player())# need to make sure whether observation = state works\n",
    "        for i in index:\n",
    "            print(\"checking move:\" + str(self.game.get_move(i)))\n",
    "            if str(self.game.get_move(i)) in str(observation.legal_moves()):# Is there a way of not using str?\n",
    "                print(\"valid\")\n",
    "                return self.game.get_move(i)\n",
    "            else:\n",
    "                print(\"invalid, checking next possible move in priority list\")\n",
    "        #here you probably need to check if it is a valid action\n",
    "        # There's a code in the HLE that gets the legal actions from a state (a \"mask\")\n",
    "        # you can do mask*model.forward()\n",
    "        # This will multiply all ILLEGAL actions by zero\n",
    "\"\"\"\n",
    "    def make_action(array):\n",
    "        index = np.rgmax(array)\n",
    "        return index\n",
    "\n",
    "\n",
    "    def get_weights():\n",
    "        #Pytorch code to get an array with all the weights of the network\n",
    "        weights  = [] #np.zeros()\n",
    "\n",
    "    def set_wegiths(weights):\n",
    "        #Pytorch code to set the weigths\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'NewGame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cd56f91ed471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgame_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"players\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_start_player\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyhanabi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHanabiGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mobs_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyhanabi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservationEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpyhanabi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservationEncoderType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCANONICAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Hanabi/aaai/hanabi-ad-hoc-learning/pytorch/pyhanabi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    710\u001b[0m       \u001b[0mc_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"char * [\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_game\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyhanabi_game_t*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m       \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnew_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'NewGame'"
     ]
    }
   ],
   "source": [
    "game_parameters = {\"players\": 2, \"random_start_player\": True}\n",
    "game = pyhanabi.HanabiGame(game_parameters)\n",
    "print(game.parameter_string(), end=\"\")\n",
    "obs_encoder = pyhanabi.ObservationEncoder(game, enc_type=pyhanabi.ObservationEncoderType.CANONICAL)\n",
    "state = game.new_initial_state()\n",
    "\n",
    "pyagent = PytorchAgent(game)\n",
    "\n",
    "\n",
    "# one run of game\n",
    "while not state.is_terminal():\n",
    "    if state.cur_player() == pyhanabi.CHANCE_PLAYER_ID:\n",
    "        state.deal_random_card()\n",
    "        continue\n",
    "\n",
    "    #print_state(state)\n",
    "\n",
    "    #observation = state.observation(state.cur_player())\n",
    "    #print_observation(observation)\n",
    "    #print(observation.legal_moves())\n",
    "    #print_encoded_observations(obs_encoder, state, game.num_players())\n",
    "    #encoded_observations = get_encoded_observations(obs_encoder, state, state.cur_player())\n",
    "    #mytensor = list_to_tensor(encoded_observations)\n",
    "    #legal_moves = state.legal_moves()\n",
    "    # print(\"\")\n",
    "    # print(\"Number of legal moves: {}\".format(len(legal_moves)))\n",
    "    current_life_token = state.life_tokens()\n",
    "    current_fireworks = state.fireworks()\n",
    "    move  = pyagent.act(state)\n",
    "    # move = np.random.choice(legal_moves)\n",
    "    # print(\"Chose random legal move: {}\".format(move))\n",
    "\n",
    "    state.apply_move(move)\n",
    "print(\"game finished\")\n",
    "print(state.fireworks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = rl_env.make('Hanabi-Full', num_players=2)\n",
    "state = environment.reset()\n",
    "while not state.is_terminal():\n",
    "    if state.cur_player() == pyhanabi.CHANCE_PLAYER_ID:  \n",
    "        state.deal_random_card()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(get_encoded_observations(obs_encoder, state, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_observations = torch.FloatTensor(get_encoded_observations(obs_encoder, state, 0))\n",
    "net(tensor_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net(tensor_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_value = net(tensor_observations).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_value.index(max(move_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_observations(encoder, state, player_num):\n",
    "    codes = encoder.encode(state.observation(player_num))\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.cur_player()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_observations = get_encoded_observations(obs_encoder, state, state.cur_player())\n",
    "mytensor = list_to_tensor(encoded_observations)\n",
    "legal_moves = state.legal_moves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.new_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.legal_moves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.life_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.fireworks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation.legal_moves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = rl_env.make('Hanabi-Full', num_players=2)\n",
    "state = environment.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = environment.reset()\n",
    "type(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyhanabi.HanabiGame()\n",
    "obs_encoder = pyhanabi.ObservationEncoder(game, enc_type=pyhanabi.ObservationEncoderType.CANONICAL)\n",
    "print(game.parameter_string(), end=\"\")\n",
    "obs_encoder = pyhanabi.ObservationEncoder(game, enc_type=pyhanabi.ObservationEncoderType.CANONICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.legal_moves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
